{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAME : MOHIT\n",
    "\n",
    "ROLL NO. : 23110207\n",
    "\n",
    "ASSIGNMENT 1\n",
    "\n",
    "Collaborators: Tanishq Bhushan Chaudhari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given Cost function\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = \\sum_{i} \\frac{1}{|C_i|} \\sum_{x,y \\in C_i} \\|x - y\\|_2^2\n",
    "$$\n",
    "\n",
    "The cost function for one cluster, \n",
    "\n",
    "$$\n",
    "S = \\sum_{x,y \\in C_i} \\|x - y\\|_2^2\n",
    "$$\n",
    "\n",
    "Expanding,\n",
    "\n",
    "$$\n",
    "S = \\sum_{x,y \\in C_i} (\\|x\\|_2^2 + \\|y\\|_2^2 - 2 x^\\top y)\n",
    "$$\n",
    "\n",
    "Splitting summations,\n",
    "\n",
    "$$\n",
    "S = \\sum_{x \\in C_i} \\sum_{y \\in C_i} \\|x\\|_2^2 + \\sum_{x \\in C_i} \\sum_{y \\in C_i} \\|y\\|_2^2 - 2 \\sum_{x \\in C_i} \\sum_{y \\in C_i} x^\\top y\n",
    "$$\n",
    "\n",
    "\n",
    "In the first term, the expression is independent from $,$ thus each sum over $y$ counts terms $ |C_i| $ times. Similarly we can rewrite the second term too and we can also substitute $y$ in the third term with expression derived from the equation of centroid or mean.\n",
    "\n",
    "$$\n",
    "S = |C_i| \\sum_{x \\in C_i} \\|x\\|_2^2 + |C_i| \\sum_{y \\in C_i} \\|y\\|_2^2 - 2 \\sum_{x \\in C_i} |C_i| x^\\top \\mu_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\sum_{x \\in C_i} \\|x\\|_2^2 - 2 |C_i| \\sum_{x \\in C_i} x^\\top \\mu_i\n",
    "$$\n",
    "\n",
    "On substituting $ x^\\top $ with expression based on $ \\mu_i $, we get\n",
    "\n",
    "$$\n",
    "\\sum_{x \\in C_i} x^\\top \\mu_i = |C_i| \\mu_i^\\top \\mu_i\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\sum_{x \\in C_i} \\|x\\|_2^2 - 2 |C_i| \\mu_i^\\top (|C_i| \\mu_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\left( \\sum_{x \\in C_i} \\|x\\|_2^2 - |C_i| \\|\\mu_i\\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Substituting,\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = 2 |C_i| \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = \\sum_{i} \\frac{1}{|C_i|} \\cdot (2 |C_i| \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2 \\sum_{i} \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "which is twice the k-means cost function,\n",
    "\n",
    "$$\n",
    "\\sum_{i} \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = 2 \\times \\text{k-means cost}\n",
    "$$\n",
    "\n",
    "so the given cost function is proportional to the k-means cost function with a factor of 2. Hence an algorithm that minimises the k-means cosr also minimises the given cost function. Thus we can directly use the K-means or K-means++ algorithm:\n",
    "\n",
    "1. Initialize k centroids based on either of the algorithms, preferably K-means++ initialization.\n",
    "2. After the centroids have been selected, assign each point to its closest center.\n",
    "3. Change the cluster center to the centroids of the clusters.\n",
    "4. Repeat the second are third step until the centroids don't change musch or untill when only few points change their cluster in an iteration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Let $OPT$ be the optimal k-means cost with centers allowed to be arbitrary points. Let $OPT'$ be the optimal k-means cost with centers can only be from data points. Let $ C_1, C_2, \\ldots, C_k $ be the clusters in the optimal solution for $OPT$, with centers $ \\mu_1, \\mu_2, \\ldots, \\mu_k $. For each cluster $ C_i $, let $ x_i \\in C_i $ be the data point closest to the center $ \\mu_i $. For any point $ y \\in C_i $, by the triangle inequality:\n",
    "     $$\n",
    "     \\|y - x_i\\| \\leq \\|y - \\mu_i\\| + \\|\\mu_i - x_i\\|.\n",
    "     $$\n",
    "Since $ x_i $ is the closest data point to $ \\mu_i $,\n",
    "     $$\n",
    "     \\|\\mu_i - x_i\\| \\leq \\|\\mu_i - y\\|\n",
    "     $$\n",
    "and, \n",
    "     $$\n",
    "     \\|y - x_i\\| \\leq 2\\|y - \\mu_i\\|.\n",
    "     $$\n",
    "\n",
    "On squaring both sides,\n",
    "     $$\n",
    "     \\|y - x_i\\|^2 \\leq 4\\|y - \\mu_i\\|^2.\n",
    "     $$\n",
    "\n",
    "Summing over all the points $ y \\in C_i $:\n",
    "     $$\n",
    "     \\sum_{y \\in C_i} \\|y - x_i\\|^2 \\leq 4\\sum_{y \\in C_i} \\|y - \\mu_i\\|^2.\n",
    "     $$\n",
    "\n",
    "Summing over all the $k$ clusters, \n",
    "     $$\n",
    "     OPT' \\leq \\sum_{i=1}^k \\sum_{y \\in C_i} \\|y - x_i\\|^2\n",
    "           \\leq 4\\sum_{i=1}^k \\sum_{y \\in C_i} \\|y - \\mu_i\\|^2\n",
    "           = 4\\cdot OPT.\n",
    "     $$\n",
    "\n",
    "Note that $OPT'$ is not necessarily equal to the cost we get by this method of choosing the center, there can be better methods by which we may be able to reduce the cost. Thus, the worst-case ratio between $OPT'$ and $OPT$ is at most 4:\n",
    "$$\n",
    "\\text{OPT'} \\leq 4\\cdot\\text{OPT}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a non-negative random variable $X$ and a positive constant $a$, Markov's inequality states that, \n",
    "$$\n",
    "P(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}.\n",
    "$$\n",
    "Here, $P$ is probability and $\\mathbb{E}[X]$ is the expectation of the random variable X.\n",
    "\n",
    "Creating a random variable $X$ such the Markov's inequality on $X$ is tight means that the Markov's Inequaluty becomes an equation. i.e.\n",
    "$$\n",
    "P(X \\geq a) = \\frac{\\mathbb{E}[X]}{a}.\n",
    "$$\n",
    "\n",
    "Let $X$ be a random variable whose probability ditribution is as such, \n",
    "$$\n",
    "X =\n",
    "\\begin{cases}\n",
    "0 & \\text{, } 1 - p \\\\\n",
    "a & \\text{, } p \n",
    "\\end{cases}\n",
    "$$\n",
    "Note that, here $a$ is a positive constant and $\\mathbb{E}[X]$ is the expectation of $X$.\n",
    "$$\n",
    "\\mathbb{E}[X] = 0*(1-p) + a*p\n",
    "$$\n",
    "This implies $X$ is a valid random variable as,\n",
    "$$\n",
    "\\mathbb{E}[X] = a*p \n",
    "$$\n",
    "\n",
    "Also, for $X$, Markov's Inequality is tight as,\n",
    "$$\n",
    "P(X \\geq a) = p = \\frac{\\mathbb{E}[X]}{a}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "In the above random variable, Markov's inequality is tight but in many other examples, other inequalities such as Chebyshev and Chernoff may give tighter bounds. Choice of inequality can also be based on the informatino available. When variance is also available, Chebyshev's inequality can be used to find deviations from $\\mu$. Also, It does not require the random variable to be non-negative and gives two sided bounds. Chernoff inequality is particularly useful when we need high-probability guarantees tail bounds on the sum of independently binary random variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape: (10000, 784)\n",
      "Testing Labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Each image tensor is flattened to a vector\n",
    "# Also each pixel value has been normalised\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "print(\"Testing data shape:\", X_test.shape)\n",
    "print(\"Testing Labels shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I made multiple tries to load the data downlaoded from the Kaggle link but for some reason it couldn't be accessed by this notebook. Thus I chose a different source for the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kpp(X, k, threshold):\n",
    "    \n",
    "    ##initialization\n",
    "    ## assigning first centroid randomly\n",
    "    centroids = [X[np.random.choice(len(X))]]   \n",
    "    for i in range(0, k-1):   # k-1 iterations\n",
    "        sqdistances = np.min(cdist(X, np.array(centroids), metric='sqeuclidean'), axis=1)  # both the inputs should be mupy arrays\n",
    "                                                                                           # axis=1 : min distance centroid for each point                     \n",
    "        probabilities = sqdistances / sqdistances.sum()\n",
    "        centroids.append( X[np.random.choice (len(X) , p=probabilities )])  # p to specify weighted distribution                  \n",
    "    centroids = np.array( centroids)\n",
    "\n",
    "    previous_labels = np.zeros(len(X))\n",
    "    \n",
    "    while True:\n",
    "        labels = np.argmin(cdist(X, centroids), axis=1)   #argmin gives the index and labels don't mean the digit just the cluster label\n",
    "        next_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])  ## K * dimension\n",
    "        \n",
    "        if np.linalg.norm(next_centroids -centroids) <threshold:   #stopping if centroids don't change much\n",
    "            break\n",
    "        centroids = next_centroids\n",
    "\n",
    "        if np.array_equal(labels, previous_labels): # Stopping if cluster assignments don’t change at all\n",
    "            break\n",
    "        previous_labels = labels\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Rand Index with convergence threshold = 0.1: 0.8897\n",
      "K-Means Rand Index with convergence threshold = 0.01: 0.8873\n",
      "K-Means Rand Index with convergence threshold = 0.001: 0.8861\n",
      "K-Means Rand Index with convergence threshold = 0.0001: 0.8873\n",
      "K-Means Rand Index with convergence threshold = 1e-05: 0.8869\n",
      "K-Means Rand Index with convergence threshold = 1e-06: 0.8848\n"
     ]
    }
   ],
   "source": [
    "thresholds = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    labels_kpp = kpp(X_test, 10, threshold)\n",
    "    print(f\"K-Means Rand Index with convergence threshold = {threshold}: {rand_score(Y_test, labels_kpp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kcenters(X, k):\n",
    "\n",
    "    centers = [X[np.random.choice(len(X))]] #choosing first centre randomly\n",
    "\n",
    "    for i in range(k - 1):\n",
    "        distances = np.min(cdist(X, np.array(centers)), axis=1) #distances to nearest centres\n",
    "        next_center = X[np.argmax(distances)] #Selecting the farthest point\n",
    "        centers.append(next_center)\n",
    "\n",
    "    centers = np.array(centers)\n",
    "    labels = np.argmin(cdist(X, centers), axis=1) #assigning the points\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Center Rand Index: 0.7072\n"
     ]
    }
   ],
   "source": [
    "labels_kcenter = kcenters(X_test, k=10)\n",
    "\n",
    "print(f\"K-Center Rand Index: {rand_score(Y_test, labels_kcenter):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Linkage Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlelinkage(X, k):\n",
    "    clusters = {i: [i] for i in range(len(X))}  # Cluster Dictionary\n",
    "    \n",
    "    distance_matrix = cdist(X, X)  # Pairwise distances between all points\n",
    "    np.fill_diagonal(distance_matrix, np.inf)  #so that we dont merge same points\n",
    "\n",
    "    while len(clusters) > k: #merging till 10 cluster remains\n",
    "        \n",
    "        min_distance = np.inf\n",
    "        min_pair = None\n",
    "        \n",
    "        cluster_ids = list(clusters.keys())\n",
    "        for i in range(len(cluster_ids)):\n",
    "            for j in range(i + 1, len(cluster_ids)):  ## so that we dompute a pair twice ## and we know that i<j\n",
    "                # Compute minimum distance between clusters\n",
    "                dist = np.min(distance_matrix[np.ix_(clusters[cluster_ids[i]], clusters[cluster_ids[j]])])\n",
    "                                             # np.ix_ \n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    min_pair = (cluster_ids[i], cluster_ids[j])\n",
    "        \n",
    "        c1, c2 = min_pair  # merging pairs\n",
    "        clusters[c1].extend(clusters[c2])  # Merge c2 into c1\n",
    "        del clusters[c2]  # Remove the merged cluster\n",
    "        \n",
    "        for c in clusters.keys():  ##updating distances\n",
    "            if c != c1:\n",
    "                dists = distance_matrix[np.ix_(clusters[c], clusters[c1])]\n",
    "                distance_matrix[c, c1] = distance_matrix[c1, c] = np.min(dists)\n",
    "\n",
    "    labels = np.zeros(len(X), dtype=int) #assigning labels\n",
    "    for id, points in enumerate(clusters.values()):\n",
    "        for p in points:\n",
    "            labels[p] = id\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Linkage Rand Index: 0.2350\n"
     ]
    }
   ],
   "source": [
    "labels_single_linkage = singlelinkage(X_test[:150], k=10)\n",
    "\n",
    "print(f\"Single Linkage Rand Index: {rand_score(Y_test[:150], labels_single_linkage):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has a time complexity of roughly $O(n^3)$ and thus I run it only for 150 datapoints. For the fulll dataset we can use the following two codes. The first one uses scipy library and the other one uses sklearn as aksed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Linkage Rand Index: 0.1017\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "Z = linkage(X_test, method='single') \n",
    "\n",
    "labels_single_linkage = fcluster(Z, 10, criterion='maxclust')\n",
    "\n",
    "print(f\"Single Linkage Rand Index: {rand_score(Y_test, labels_single_linkage):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Linkage Rand Index: 0.1017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=10, linkage=\"single\")\n",
    "labels_agg = agg_clustering.fit_predict(X_test)\n",
    "\n",
    "rand_index = rand_score(Y_test, labels_agg)\n",
    "print(f\"Single Linkage Rand Index: {rand_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-k approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svd(A, k):\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)  # Full SVD\n",
    "    \n",
    "    U_k = U[:, :k]  # First k columns of U\n",
    "    S_k = np.diag(S[:k])  # First k singular values in diagonal form\n",
    "                          # S is an array of singular values in descending order\n",
    "    return U_k @ S_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Rand Index for rank-2 approximation: 0.8312\n",
      "K-Center Rand Index for rank-2 approximation: 0.6872\n",
      "K-Means Rand Index for rank-5 approximation: 0.8673\n",
      "K-Center Rand Index for rank-5 approximation: 0.7815\n",
      "K-Means Rand Index for rank-10 approximation: 0.8802\n",
      "K-Center Rand Index for rank-10 approximation: 0.7752\n"
     ]
    }
   ],
   "source": [
    "k_values = [2, 5, 10]\n",
    "tolerance = 1e-5  \n",
    "\n",
    "for k in k_values:\n",
    "    X_reduced = compute_svd(X_test, k)\n",
    "    \n",
    "    labels_kpp = kpp(X_reduced, 10, tolerance)  ## K-means\n",
    "    print(f\"K-Means Rand Index for rank-{k} approximation: {rand_score(Y_test, labels_kpp):.4f}\")\n",
    "  \n",
    "    labels_kcenter = kcenters(X_reduced, k=10)  ## K-centers\n",
    "    print(f\"K-Center Rand Index for rank-{k} approximation: {rand_score(Y_test, labels_kcenter):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the estimate of this function by sampling on $n$ people from the population and using the following calculations. \n",
    "\n",
    "Lets create a random variable $X_i$ s.t.\n",
    "\n",
    "$$\n",
    "X_i = \\begin{cases}\n",
    "1, & \\text{if the $i^{th}$ sampled person drinks coffee,} \\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "for $i = 1, 2, \\dots, n$, where $n$ is the number of samples we take.\n",
    "\n",
    "Let $S$ be a new random variable s.t.\n",
    "\n",
    "$$\n",
    "S = X_1 + X_2 + \\cdots + X_n.\n",
    "$$\n",
    "\n",
    "Then the estimator for the fraction $p$ of coffee drinkers is:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{S}{n}.\n",
    "$$\n",
    "\n",
    "This is because $\\mathbb{E}[X] = \\hat{p}$ and $\\mathbb{E}[S]= n *\\hat{p}$.\n",
    "\n",
    "From adding upper and lower tail inequalities from Chernoff's inequalities we get,\n",
    "$$\n",
    "P\\Bigl(|S - np| \\ge \\delta np\\Bigr) \\le 2 \\exp\\Bigl(-\\frac{\\delta^2 np}{2}\\Bigr).\n",
    "$$\n",
    "\n",
    "Dividing the inequality inside the probability function with %np$ we get,\n",
    "$$\n",
    "P\\left(\\frac{|\\hat{p} - p|}{p} \\ge \\delta\\right) \\le 2 \\exp\\Bigl(-\\frac{\\delta^2 \\, np}{2}\\Bigr).\n",
    "$$\n",
    "\n",
    "We want this probability of relative error being in such tolerable range to be $0.99$, thus the above probabilty should be less than $0.01$ i.e.\n",
    "\n",
    "$$\n",
    "P\\left(\\frac{|\\hat{p} - p|}{p} \\ge \\delta\\right) \\le 0.01,\n",
    "$$\n",
    "\n",
    "This inquality will be fulfilled automatically if the following is followed, \n",
    "\n",
    "$$\n",
    "2 \\exp\\Bigl(-\\frac{\\delta^2 \\, np}{2}\\Bigr) \\le 0.01.\n",
    "$$\n",
    "\n",
    "Taking log with base $e$ of both sides after dividing by 2, \n",
    "\n",
    "$$\n",
    "-\\frac{\\delta^2 \\, np}{2} \\le \\ln(0.005).\n",
    "$$\n",
    "\n",
    "By using calculator, we get $\\ln(0.005) \\approx -5.298$, and thus we have,\n",
    "\n",
    "$$\n",
    "\\frac{\\delta^2 \\, np}{2} \\ge 5.298.\n",
    "$$\n",
    "\n",
    "Getting n:\n",
    "\n",
    "$$\n",
    "n \\ge \\frac{10.596}{\\delta^2 \\, p}.\n",
    "$$\n",
    "\n",
    "At least 1% of the population drinks coffee, so $p \\ge 0.01$. Thus,\n",
    "\n",
    "$$\n",
    "n \\ge \\frac{10.596}{\\delta^2 \\times 0.01} = \\frac{10.596}{0.01 \\, \\delta^2} = \\frac{1059.6}{\\delta^2}.\n",
    "$$\n",
    "\n",
    "In all the above equations, $\\delta$ is relative error. We can choose this by ourselves depending on how close we want our results to the actual probabilty. Similarly we also reduce it further we can reduce the confidence. Thus, let this relative error be 10%, or $\\delta=0.1$. \n",
    "\n",
    "$$\n",
    "n \\ge \\frac{1059.6}{(0.1)^2} = \\frac{1059.6}{0.01} \\approx 105,960.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Conclusion :\n",
    "\n",
    "\n",
    "To estimate the fraction of coffee drinkers with a 10% relative error and 99% confidence, you need to sample at least 105,960 individuals from the population. We can also reduce the sample size of our experiment if we are okay with some more error with the same confidence.\n",
    "\n",
    "The Algorithm:\n",
    "\n",
    "The following is the algorithm for finding the estimate with a relative error of 10% with 99% confidence:\n",
    "1. Sample at least 105,960 individuals from the population.\n",
    "2. Form the random variable $S$ and get $\\hat{p}$ as discussed above.\n",
    "3. If we multiply $\\hat{p}$ with 1,000,000 we will get the estimate of the population of coffee drinkers with a relative error of 10% with 99% confidence.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
