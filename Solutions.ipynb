{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAME : MOHIT\n",
    "\n",
    "ROLL NO. : 23110207\n",
    "\n",
    "ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given Cost function\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = \\sum_{i} \\frac{1}{|C_i|} \\sum_{x,y \\in C_i} \\|x - y\\|_2^2\n",
    "$$\n",
    "\n",
    "The cost function for one cluster, \n",
    "\n",
    "$$\n",
    "S = \\sum_{x,y \\in C_i} \\|x - y\\|_2^2\n",
    "$$\n",
    "\n",
    "Expanding,\n",
    "\n",
    "$$\n",
    "S = \\sum_{x,y \\in C_i} (\\|x\\|_2^2 + \\|y\\|_2^2 - 2 x^\\top y)\n",
    "$$\n",
    "\n",
    "Splitting summations,\n",
    "\n",
    "$$\n",
    "S = \\sum_{x \\in C_i} \\sum_{y \\in C_i} \\|x\\|_2^2 + \\sum_{x \\in C_i} \\sum_{y \\in C_i} \\|y\\|_2^2 - 2 \\sum_{x \\in C_i} \\sum_{y \\in C_i} x^\\top y\n",
    "$$\n",
    "\n",
    "\n",
    "In the first term, the expression is independent from $,$ thus each sum over $y$ counts terms $ |C_i| $ times. Similarly we can rewrite the second term too and we can also substitute $y$ in the third term with expression derived from the equation of centroid or mean.\n",
    "\n",
    "$$\n",
    "S = |C_i| \\sum_{x \\in C_i} \\|x\\|_2^2 + |C_i| \\sum_{y \\in C_i} \\|y\\|_2^2 - 2 \\sum_{x \\in C_i} |C_i| x^\\top \\mu_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\sum_{x \\in C_i} \\|x\\|_2^2 - 2 |C_i| \\sum_{x \\in C_i} x^\\top \\mu_i\n",
    "$$\n",
    "\n",
    "On substituting $ x^\\top $ with expression based on $ \\mu_i $, we get\n",
    "\n",
    "$$\n",
    "\\sum_{x \\in C_i} x^\\top \\mu_i = |C_i| \\mu_i^\\top \\mu_i\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\sum_{x \\in C_i} \\|x\\|_2^2 - 2 |C_i| \\mu_i^\\top (|C_i| \\mu_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\left( \\sum_{x \\in C_i} \\|x\\|_2^2 - |C_i| \\|\\mu_i\\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = 2 |C_i| \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Substituting,\n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in C_i} \\|x - y\\|_2^2 = 2 |C_i| \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = \\sum_{i} \\frac{1}{|C_i|} \\cdot (2 |C_i| \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2 \\sum_{i} \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "which is twice the k-means cost function,\n",
    "\n",
    "$$\n",
    "\\sum_{i} \\sum_{x \\in C_i} \\|x - \\mu_i\\|_2^2\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\text{cost}(\\mathcal{C}) = 2 \\times \\text{k-means cost}\n",
    "$$\n",
    "\n",
    "so the given cost function is proportional to the k-means cost function with a factor of 2. Hence an algorithm that minimises the k-means cosr also minimises the given cost function. Thus we can directly use the K-means or K-means++ algorithm:\n",
    "\n",
    "1. Initialize k centroids based on either of the algorithms, preferably K-means++ initialization.\n",
    "2. After the centroids have been selected, assign each point to its closest center.\n",
    "3. Change the cluster center to the centroids of the clusters.\n",
    "4. Repeat the second are third step until the centroids don't change musch or untill when only few points change their cluster in an iteration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Let $OPT$ be the optimal k-means cost with centers allowed to be arbitrary points. Let $OPT'$ be the optimal k-means cost with centers can only be from data points. Let $ C_1, C_2, \\ldots, C_k $ be the clusters in the optimal solution for $OPT$, with centers $ \\mu_1, \\mu_2, \\ldots, \\mu_k $. For each cluster $ C_i $, let $ x_i \\in C_i $ be the data point closest to the center $ \\mu_i $. For any point $ y \\in C_i $, by the triangle inequality:\n",
    "     $$\n",
    "     \\|y - x_i\\| \\leq \\|y - \\mu_i\\| + \\|\\mu_i - x_i\\|.\n",
    "     $$\n",
    "Since $ x_i $ is the closest data point to $ \\mu_i $,\n",
    "     $$\n",
    "     \\|\\mu_i - x_i\\| \\leq \\|\\mu_i - y\\|\n",
    "     $$\n",
    "and, \n",
    "     $$\n",
    "     \\|y - x_i\\| \\leq 2\\|y - \\mu_i\\|.\n",
    "     $$\n",
    "\n",
    "On squaring both sides,\n",
    "     $$\n",
    "     \\|y - x_i\\|^2 \\leq 4\\|y - \\mu_i\\|^2.\n",
    "     $$\n",
    "\n",
    "Summing over all the points $ y \\in C_i $:\n",
    "     $$\n",
    "     \\sum_{y \\in C_i} \\|y - x_i\\|^2 \\leq 4\\sum_{y \\in C_i} \\|y - \\mu_i\\|^2.\n",
    "     $$\n",
    "\n",
    "Summing over all the $k$ clusters, \n",
    "     $$\n",
    "     OPT' \\leq \\sum_{i=1}^k \\sum_{y \\in C_i} \\|y - x_i\\|^2\n",
    "           \\leq 4\\sum_{i=1}^k \\sum_{y \\in C_i} \\|y - \\mu_i\\|^2\n",
    "           = 4\\cdot OPT.\n",
    "     $$\n",
    "\n",
    "Note that $OPT'$ is not necessarily equal to the cost we get by this method of choosing the center, there can be better methods by which we may be able to reduce the cost. Thus, the worst-case ratio between $OPT'$ and $OPT$ is at most 4:\n",
    "$$\n",
    "\\text{OPT'} \\leq 4\\cdot\\text{OPT}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a non-negative random variable $X$ and a positive constant $a$, Markov's inequality states that, \n",
    "$$\n",
    "P(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}.\n",
    "$$\n",
    "Here, $P$ is probability and $\\mathbb{E}[X]$ is the expectation of the random variable X.\n",
    "\n",
    "Creating a random variable $X$ such the Markov's inequality on $X$ is tight means that the Markov's Inequaluty becomes an equation. i.e.\n",
    "$$\n",
    "P(X \\geq a) = \\frac{\\mathbb{E}[X]}{a}.\n",
    "$$\n",
    "\n",
    "Let $X$ be a random variable whose probability ditribution is as such, \n",
    "$$\n",
    "X =\n",
    "\\begin{cases}\n",
    "0 & \\text{, } 1 - p \\\\\n",
    "a & \\text{, } p = \\frac{\\mathbb{E}[X]}{a} \n",
    "\\end{cases}\n",
    "$$\n",
    "Note that, here $a$ is a positive constant and $\\mathbb{E}[X]$ is the expectation of $X$.\n",
    "$$\n",
    "\\mathbb{E}[X] = 0*(1-p) + a*\\frac{\\mathbb{E}[X]}{a}\n",
    "$$\n",
    "This implies $X$ is a valid random variable as,\n",
    "$$\n",
    "\\mathbb{E}[X] =\\mathbb{E}[X] \n",
    "$$\n",
    "\n",
    "Also, for $X$, Markov's Inequality is tight as,\n",
    "$$\n",
    "P(X \\geq a) = \\frac{\\mathbb{E}[X]}{a}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "In the above random variable, Markov's inequality is tight but in many other examples, other inequalities such as Chebyshev and Chernoff may give tighter bounds. Choice of inequality can also be based on the informatino available. When variance is also available, Chebyshev's inequality can be used to find deviations from $\\mu$. Also, It does not require the random variable to be non-negative and gives two sided bounds. Chernoff inequality is particularly useful when we need high-probability guarantees tail bounds on the sum of independently binary random variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data shape: (10000, 784)\n",
      "Testing Labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Each image tensor is flattened to a vector\n",
    "# Also each pixel value has been normalised\n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "print(\"Testing data shape:\", X_test.shape)\n",
    "print(\"Testing Labels shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kpp(X, k, threshold):\n",
    "    \n",
    "    ##initialization\n",
    "    ## assigning first centroid randomly\n",
    "    centroids = [X[np.random.choice(len(X))]]   \n",
    "    for i in range(0, k-1):   # k-1 iterations\n",
    "        sqdistances = np.min(cdist(X, np.array(centroids), metric='sqeuclidean'), axis=1)  # both the inputs should be mupy arrays\n",
    "                                                                                           # axis=1 : min distance centroid for each point                     \n",
    "        probabilities = sqdistances / sqdistances.sum()\n",
    "        centroids.append( X[np.random.choice (len(X) , p=probabilities )])  # p to specify weighted distribution                  \n",
    "    centroids = np.array( centroids)\n",
    "\n",
    "    previous_labels = np.zeros(len(X))\n",
    "    \n",
    "    while True:\n",
    "        labels = np.argmin(cdist(X, centroids), axis=1)   #argmin gives the index and labels don't mean the digit just the cluster label\n",
    "        next_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])  ## K * dimension\n",
    "        \n",
    "        if np.linalg.norm(next_centroids -centroids) <threshold:   #stopping if centroids don't change much\n",
    "            break\n",
    "        centroids = next_centroids\n",
    "\n",
    "        if np.array_equal(labels, previous_labels): # Stopping if cluster assignments donâ€™t change at all\n",
    "            break\n",
    "        previous_labels = labels\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Rand Index with convergence threshold = 0.1: 0.8897\n",
      "K-Means Rand Index with convergence threshold = 0.01: 0.8873\n",
      "K-Means Rand Index with convergence threshold = 0.001: 0.8861\n",
      "K-Means Rand Index with convergence threshold = 0.0001: 0.8873\n",
      "K-Means Rand Index with convergence threshold = 1e-05: 0.8869\n",
      "K-Means Rand Index with convergence threshold = 1e-06: 0.8848\n"
     ]
    }
   ],
   "source": [
    "thresholds = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    labels_kpp = kpp(X_test, 10, threshold)\n",
    "    print(f\"K-Means Rand Index with convergence threshold = {threshold}: {rand_score(Y_test, labels_kpp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kcenters(X, k):\n",
    "\n",
    "    centers = [X[np.random.choice(len(X))]] #choosing first centre randomly\n",
    "\n",
    "    for i in range(k - 1):\n",
    "        distances = np.min(cdist(X, np.array(centers)), axis=1) #distances to nearest centres\n",
    "        next_center = X[np.argmax(distances)] #Selecting the farthest point\n",
    "        centers.append(next_center)\n",
    "\n",
    "    centers = np.array(centers)\n",
    "    labels = np.argmin(cdist(X, centers), axis=1) #assigning the points\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Center Rand Index: 0.7072\n"
     ]
    }
   ],
   "source": [
    "labels_kcenter = kcenters(X_test, k=10)\n",
    "\n",
    "print(f\"K-Center Rand Index: {rand_score(Y_test, labels_kcenter):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Linkage Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlelinkage(X, k):\n",
    "    clusters = {i: [i] for i in range(len(X))}  # Cluster Dictionary\n",
    "    \n",
    "    distance_matrix = cdist(X, X)  # Pairwise distances between all points\n",
    "    np.fill_diagonal(distance_matrix, np.inf)  #so that we dont merge same points\n",
    "\n",
    "    while len(clusters) > k: #merging till 10 cluster remains\n",
    "        \n",
    "        min_distance = np.inf\n",
    "        min_pair = None\n",
    "        \n",
    "        cluster_ids = list(clusters.keys())\n",
    "        for i in range(len(cluster_ids)):\n",
    "            for j in range(i + 1, len(cluster_ids)):  ## so that we dompute a pair twice ## and we know that i<j\n",
    "                # Compute minimum distance between clusters\n",
    "                dist = np.min(distance_matrix[np.ix_(clusters[cluster_ids[i]], clusters[cluster_ids[j]])])\n",
    "                                             # np.ix_ \n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    min_pair = (cluster_ids[i], cluster_ids[j])\n",
    "        \n",
    "        c1, c2 = min_pair  # merging pairs\n",
    "        clusters[c1].extend(clusters[c2])  # Merge c2 into c1\n",
    "        del clusters[c2]  # Remove the merged cluster\n",
    "        \n",
    "        for c in clusters.keys():  ##updating distances\n",
    "            if c != c1:\n",
    "                dists = distance_matrix[np.ix_(clusters[c], clusters[c1])]\n",
    "                distance_matrix[c, c1] = distance_matrix[c1, c] = np.min(dists)\n",
    "\n",
    "    labels = np.zeros(len(X), dtype=int) #assigning labels\n",
    "    for id, points in enumerate(clusters.values()):\n",
    "        for p in points:\n",
    "            labels[p] = id\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Linkage Rand Index: 0.2350\n"
     ]
    }
   ],
   "source": [
    "labels_single_linkage = singlelinkage(X_test[:150], k=10)\n",
    "\n",
    "print(f\"Single Linkage Rand Index: {rand_score(Y_test[:150], labels_single_linkage):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Linkage Rand Index: 0.1017\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "Z = linkage(X_test, method='single') \n",
    "\n",
    "labels_single_linkage = fcluster(Z, 10, criterion='maxclust')\n",
    "\n",
    "print(f\"Single Linkage Rand Index: {rand_score(Y_test, labels_single_linkage):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-k approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svd(A, k):\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)  # Full SVD\n",
    "    \n",
    "    U_k = U[:, :k]  # First k columns of U\n",
    "    S_k = np.diag(S[:k])  # First k singular values in diagonal form\n",
    "                          # S is an array of singular values in descending order\n",
    "    return U_k @ S_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Rand Index for rank-2 approximation: 0.8312\n",
      "K-Center Rand Index for rank-2 approximation: 0.6872\n",
      "K-Means Rand Index for rank-5 approximation: 0.8673\n",
      "K-Center Rand Index for rank-5 approximation: 0.7815\n",
      "K-Means Rand Index for rank-10 approximation: 0.8802\n",
      "K-Center Rand Index for rank-10 approximation: 0.7752\n"
     ]
    }
   ],
   "source": [
    "k_values = [2, 5, 10]\n",
    "tolerance = 1e-5  \n",
    "\n",
    "for k in k_values:\n",
    "    X_reduced = compute_svd(X_test, k)\n",
    "    \n",
    "    labels_kpp = kpp(X_reduced, 10, tolerance)  ## K-means\n",
    "    print(f\"K-Means Rand Index for rank-{k} approximation: {rand_score(Y_test, labels_kpp):.4f}\")\n",
    "  \n",
    "    labels_kcenter = kcenters(X_reduced, k=10)  ## K-centers\n",
    "    print(f\"K-Center Rand Index for rank-{k} approximation: {rand_score(Y_test, labels_kcenter):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
